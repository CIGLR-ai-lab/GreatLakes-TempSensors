{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "57ba8d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.captureWarnings(True)\n",
    "\n",
    "import deepsensor.torch\n",
    "from deepsensor.model import ConvNP\n",
    "from deepsensor.data import DataProcessor, TaskLoader, construct_circ_time_ds\n",
    "from deepsensor.data.sources import get_era5_reanalysis_data, get_earthenv_auxiliary_data, get_gldas_land_mask\n",
    "from deepsensor.train import set_gpu_default_device\n",
    "from deepsensor.train import Trainer\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6ecfed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat15 ='/nfs/turbo/seas-dannes/SST-sensor-placement-input/GLSEA3_NETCDF/GLSEA3_2015.nc'\n",
    "dat14 ='/nfs/turbo/seas-dannes/SST-sensor-placement-input/GLSEA3_NETCDF/GLSEA3_2014.nc'\n",
    "dat16 ='/nfs/turbo/seas-dannes/SST-sensor-placement-input/GLSEA3_NETCDF/GLSEA3_2016.nc'\n",
    "mask = xr.open_dataset('/home/erinredd/CIGLRProj/lakemask2.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "429de0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = xr.open_mfdataset([dat14, dat15, dat16],\n",
    "                                concat_dim='time',\n",
    "                                combine='nested',\n",
    "                                chunks={'lat': 'auto', 'lon': 'auto'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6be8826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdat = dat.where(np.isnan(dat.sst) == False, -0.009)\n",
    "climatology = mdat.groupby('time.dayofyear').mean('time')\n",
    "anomalies = mdat.groupby('time.dayofyear') - climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c8e807bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DataProcessor(x1_name=\"lat\", x2_name=\"lon\")\n",
    "anom_ds = data_processor(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7e08b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_loader = TaskLoader(\n",
    "    context = anom_ds,\n",
    "    target = anom_ds, \n",
    "#     aux=auxiliary_data_placeholder\n",
    ")\n",
    "val_tasks = []\n",
    "for date in pd.date_range('2016-01-01T12:00:00.000000000', '2016-12-31T12:00:00.000000000'):\n",
    "    N_context = np.random.randint(0, 100)\n",
    "    task = task_loader(date, context_sampling=\"all\", target_sampling=\"all\")\n",
    "    val_tasks.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2af8aa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_yc inferred from TaskLoader: (1,)\n",
      "dim_yt inferred from TaskLoader: 1\n",
      "dim_aux_t inferred from TaskLoader: 0\n",
      "internal_density inferred from TaskLoader: 1180\n",
      "encoder_scales inferred from TaskLoader: [0.00042372880852781236]\n",
      "decoder_scale inferred from TaskLoader: 0.000847457627118644\n"
     ]
    }
   ],
   "source": [
    "model = ConvNP(data_processor, task_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dd2f038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_rmse(model, val_tasks):\n",
    "    errors = []\n",
    "    target_var_ID = task_loader.target_var_IDs[0][0]  # assume 1st target set and 1D\n",
    "    for task in np.random.choice(val_tasks, 50, replace = False):\n",
    "#         print(\"im in for loop\")\n",
    "        mean = data_processor.map_array(model.mean(task), target_var_ID, unnorm=True)\n",
    "#         print(\"mean calc\")\n",
    "        true = data_processor.map_array(task[\"Y_t\"][0], target_var_ID, unnorm=True)\n",
    "#         print(\"true calc\")\n",
    "        errors.extend(np.abs(mean - true))\n",
    "    return np.sqrt(np.mean(np.concatenate(errors) ** 2))\n",
    "def gen_tasks(dates, progress=True):\n",
    "    tasks = []\n",
    "    for date in notebook.tqdm(dates, disable=not progress):\n",
    "#         N_c = np.random.randint(0, 500)\n",
    "        task = task_loader(date, context_sampling=[\"all\"], target_sampling=\"all\")\n",
    "        tasks.append(task)\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "128f8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_gpu_default_device()\n",
    "losses = []\n",
    "val_rmses = []\n",
    "train_range = pd.date_range('2015-01-02T12:00:00.000000000', '2015-12-31T12:00:00.000000000')\n",
    "val_range = pd.date_range('2016-01-01T12:00:00.000000000', '2016-12-31T12:00:00.000000000')\n",
    "val_rmse_best = np.inf\n",
    "trainer = Trainer(model, lr=5e-5)\n",
    "for epoch in range(5):\n",
    "#     print(\"step1\")\n",
    "    train_tasks = gen_tasks(pd.date_range(train_range[0], train_range[1])[::5], progress=False)\n",
    "\n",
    "    batch_losses = trainer(train_tasks)\n",
    "#     print(\"step3\")\n",
    "    losses.append(np.mean(batch_losses))\n",
    "    val_rmses.append(compute_val_rmse(model, val_tasks))  \n",
    "    if val_rmses[-1] < val_rmse_best:\n",
    "        val_rmse_best = val_rmses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1a5a65e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsensor.active_learning import GreedyAlgorithm\n",
    "\n",
    "alg = GreedyAlgorithm(\n",
    "    model,\n",
    "    X_s = anomalies,\n",
    "    X_t = anomalies,\n",
    "    context_set_idx=0,\n",
    "    target_set_idx=0,\n",
    "    N_new_context=3,\n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "594638ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsensor.active_learning.acquisition_fns import Stddev\n",
    "\n",
    "acquisition_fn = Stddev(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a423f3",
   "metadata": {},
   "source": [
    "The cell below consistently runs the alg() for 33% and then gives the error \"GriddedDataError: Cannot append to gridded data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ad3cd6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 74/222 [01:35<03:11,  1.29s/it]\n"
     ]
    },
    {
     "ename": "GriddedDataError",
     "evalue": "Cannot append to gridded data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGriddedDataError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m placement_dates \u001b[38;5;241m=\u001b[39m val_dates\n\u001b[1;32m      3\u001b[0m placement_tasks \u001b[38;5;241m=\u001b[39m task_loader(placement_dates, context_sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m X_new_df, acquisition_fn_ds \u001b[38;5;241m=\u001b[39m \u001b[43malg\u001b[49m\u001b[43m(\u001b[49m\u001b[43macquisition_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacement_tasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepsensor_env_gpu/lib/python3.10/site-packages/deepsensor/active_learning/algorithms.py:538\u001b[0m, in \u001b[0;36mGreedyAlgorithm.__call__\u001b[0;34m(self, acquisition_fn, tasks, diff)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks):\n\u001b[1;32m    532\u001b[0m     y_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_y_infill(\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposed_infill,\n\u001b[1;32m    534\u001b[0m         time\u001b[38;5;241m=\u001b[39mtask[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    535\u001b[0m         x1\u001b[38;5;241m=\u001b[39mx_new[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    536\u001b[0m         x2\u001b[38;5;241m=\u001b[39mx_new[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    537\u001b[0m     )\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks[i] \u001b[38;5;241m=\u001b[39m \u001b[43mappend_obs_to_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_set_idx\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# Append new proposed context points to dataframe\u001b[39;00m\n\u001b[1;32m    543\u001b[0m x_new_unnorm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdata_processor\u001b[38;5;241m.\u001b[39mmap_coord_array(\n\u001b[1;32m    544\u001b[0m     x_new, unnorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    545\u001b[0m )\n",
      "File \u001b[0;32m~/deepsensor_env_gpu/lib/python3.10/site-packages/deepsensor/data/task.py:377\u001b[0m, in \u001b[0;36mappend_obs_to_task\u001b[0;34m(task, X_new, Y_new, context_set_idx)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TaskSetIndexError(context_set_idx, \u001b[38;5;28mlen\u001b[39m(task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_c\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_c\u001b[39m\u001b[38;5;124m\"\u001b[39m][context_set_idx], \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GriddedDataError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot append to gridded data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    379\u001b[0m task_with_new \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(task)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Y_new\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;66;03m# Add size-1 observation and data dimension\u001b[39;00m\n",
      "\u001b[0;31mGriddedDataError\u001b[0m: Cannot append to gridded data"
     ]
    }
   ],
   "source": [
    "val_dates = pd.date_range('2016-01-01T12:00:00.000000000', '2016-12-31T12:00:00.000000000')[::5]\n",
    "placement_dates = val_dates\n",
    "placement_tasks = task_loader(placement_dates, context_sampling=\"all\")\n",
    "\n",
    "X_new_df, acquisition_fn_ds = alg(acquisition_fn, placement_tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepsensor_env_gpu)",
   "language": "python",
   "name": "deepsensor_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
